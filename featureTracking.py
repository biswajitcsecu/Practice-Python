import warnings

import cv2
import numpy as np

warnings.filterwarnings('ignore')


def calculate_region_of_interest(frame, tracking_paths):
    mask = np.zeros_like(frame)
    mask[:] = 255
    for x, y in [np.int32(tp[-1]) for tp in tracking_paths]:
        cv2.circle(mask, (x, y), 6, 0, -1)
    return mask


def add_tracking_paths(frame, tracking_paths):
    mask = calculate_region_of_interest(frame, tracking_paths)
    feature_points = cv2.goodFeaturesToTrack(frame, mask=mask, maxCorners=500,
                                             qualityLevel=0.3, minDistance=7, blockSize=7)

    if feature_points is not None:
        for x, y in np.float32(feature_points).reshape(-1, 2):
            tracking_paths.append([(x, y)])


def compute_feature_points(tracking_paths, prev_img, current_img):
    feature_points = [tp[-1] for tp in tracking_paths]
    # Vector of 2D points for which the flow needs to be found
    feature_points_0 = np.float32(feature_points).reshape(-1, 1, 2)

    feature_points_1, status_1, err_1 = cv2.calcOpticalFlowPyrLK(prev_img, current_img,
                                               feature_points_0, None, **tracking_params)
    feature_points_0_rev, status_2, err_2 = cv2.calcOpticalFlowPyrLK(current_img, prev_img,
                                                 feature_points_1, None, **tracking_params)

    # Compute the difference of the feature points
    diff_feature_points = abs(feature_points_0 - feature_points_0_rev).reshape(-1, 2).max(-1)

    # threshold and keep only the good points
    good_points = diff_feature_points < 1
    return feature_points_1.reshape(-1, 2), good_points


def start_tracking(cap, scaling_factor, num_frames_to_track, num_frames_jump, tracking_params):
    tracking_paths = []
    frame_index = 0

    # Iterate until the user presses the ESC key
    while True:
        # read the input frame
        ret, frame = cap.read()

        # downsample the input frame
        frame = cv2.resize(frame, None, fx=scaling_factor,
                           fy=scaling_factor, interpolation=cv2.INTER_AREA)

        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        output_img = frame.copy()

        if len(tracking_paths) > 0:
            prev_img, current_img = prev_gray, frame_gray
            feature_points, good_points = compute_feature_points(tracking_paths,
                                                                 prev_img, current_img)

            new_tracking_paths = []
            for tp, (x, y), good_points_flag in \
                    zip(tracking_paths, feature_points, good_points):
                if not good_points_flag: continue

                tp.append((x, y))

                # Using the queue structure i.e. first in, first out
                if len(tp) > num_frames_to_track: del tp[0]

                new_tracking_paths.append(tp)

                # draw green circles on top of the output image
                cv2.circle(output_img, (x, y), 3, (0, 255, 0), -1)

            tracking_paths = new_tracking_paths

            # draw green lines on top of the output image
            point_paths = [np.int32(tp) for tp in tracking_paths]
            cv2.polylines(output_img, point_paths, False, (0, 150, 0))

            # 'if' condition to skip every 'n'th frame
        if not frame_index % num_frames_jump:
            add_tracking_paths(frame_gray, tracking_paths)

        frame_index += 1
        prev_gray = frame_gray


        cv2.imshow('Optical Flow', output_img)


        # Check if the user pressed the ESC key
        c = cv2.waitKey(1)
        if c == 27:
            break


if __name__ == '__main__':
    # Capture the input frame
    cap = cv2.VideoCapture('rouen_video.avi')
    scaling_factor = 1.0
    num_frames_to_track = 5
    num_frames_jump = 2
    tracking_params = dict(winSize=(7, 7), maxLevel=2,
                           criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

    start_tracking(cap, scaling_factor, num_frames_to_track,num_frames_jump, tracking_params)
    cv2.destroyAllWindows()